{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who Are They?\n",
    "\n",
    "### This project will be scraping images or actors and actresses from Google Images to create a dataset that will then be used in a facial recognition project to identify actors and actresses in movies. This notebook only deals with the collection and labling of faces and the building of the dataset. The data will be stored in an S3 bucket.\n",
    "\n",
    "\n",
    "### *By: Connor Secen*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup:\n",
    "#### This is the initial setup of the notebook, importing all the packages needed and setting main variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all required imports\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import hashlib\n",
    "import multiprocessing\n",
    "import requests\n",
    "import cv2\n",
    "import config\n",
    "import boto3\n",
    "import time\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')   # instance of aws S3\n",
    "\n",
    "# preset variables \n",
    "max_results = 350   # number of images collected\n",
    "actor_num = 4   # represents number of pages == (n-1)*100 actors\n",
    "group_size = 5   # number of actors images collected at once\n",
    "imdb_url = 'https://www.imdb.com/list/ls058011111/?page={0}'   # imdb top 1000 actors webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect actors names\n",
    "#### Because we are building a dataset of actors images, we needed to first collect names of a fair number of actors. In order to do this, I will scrape the first few pages of IMDbs list of top 1000 actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapes specified page of imdbs list of top 1000 actors\n",
    "def collect_actors(url, actors):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    curr_actors = [h3.find('a').text for h3 in soup.find_all('h3', attrs={'class':'lister-item-header'})]   # get the name of each actor and add it to the list parameter\n",
    "    actors += curr_actors\n",
    "    return actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "manager = multiprocessing.Manager()\n",
    "actors = manager.list()   # list of actors names shared between threads\n",
    "jobs = []\n",
    "# start 3 process each scraping a different page of the imdbs top 1000 actors list\n",
    "for i in range(1, actor_num):\n",
    "    p = multiprocessing.Process(target=collect_actors, args=(imdb_url.format(i), actors))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "\n",
    "# wait for each process to complete\n",
    "for proc in jobs:\n",
    "    proc.join()\n",
    "\n",
    "actors = list(set(actors))   # remove any duplicate names\n",
    "actors = [a[1:-1] for a in actors]   # remove leading space and trailing newline from each name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect images\n",
    "#### Now that we have collected the names of the top 300 actors from imdbs top 1000 actors list, we can begin to collect images by scraping Google Images for each actor in the list inorder to build our dataset that will later be used to train a facial recognition cnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_image_urls(query, max_links_to_fetch, wd, sleep_between_interactions:int=1):\n",
    "    def scroll_to_end(wd):\n",
    "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(sleep_between_interactions)    \n",
    "    \n",
    "    # build the google query\n",
    "    search_url = \"https://www.google.com/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img\"\n",
    "\n",
    "    # load the page\n",
    "    wd.get(search_url.format(q=query))\n",
    "\n",
    "    image_urls = set()\n",
    "    image_count = 0\n",
    "    results_start = 0\n",
    "    while image_count < max_links_to_fetch:\n",
    "        scroll_to_end(wd)\n",
    "\n",
    "        # get all image thumbnail results\n",
    "        thumbnail_results = wd.find_elements_by_css_selector(\"img.rg_ic\")\n",
    "        number_results = len(thumbnail_results)\n",
    "                \n",
    "        for img in thumbnail_results[results_start:number_results]:\n",
    "            # try to click every thumbnail such that we can get the real image behind it\n",
    "            try:\n",
    "                img.click()\n",
    "                time.sleep(sleep_between_interactions)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            # extract image urls    \n",
    "            actual_images = wd.find_elements_by_css_selector('img.irc_mi')\n",
    "            for actual_image in actual_images:\n",
    "                if actual_image.get_attribute('src'):\n",
    "                    image_urls.add(actual_image.get_attribute('src'))\n",
    "\n",
    "            image_count = len(image_urls)\n",
    "\n",
    "            if len(image_urls) >= max_links_to_fetch:\n",
    "                print(f\"Found: {len(image_urls)} image links, done!\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Found:\", len(image_urls), \"image links, looking for more ...\")\n",
    "            time.sleep(1)\n",
    "            load_more_button = wd.find_element_by_css_selector(\".ksb\")\n",
    "            if load_more_button:\n",
    "                wd.execute_script(\"document.querySelector('.ksb').click();\")\n",
    "\n",
    "        # move the result startpoint further down\n",
    "        results_start = len(thumbnail_results)\n",
    "\n",
    "    return image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_image(folder_path:str,url:str):\n",
    "    try:\n",
    "        image_content = requests.get(url).content\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not download {url} - {e}\")\n",
    "\n",
    "    try:\n",
    "        image_file = io.BytesIO(image_content)\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n",
    "        with open(file_path, 'wb') as f:\n",
    "            image.save(f, \"JPEG\", quality=85)\n",
    "        print(f\"SUCCESS - saved {url} - as {file_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not save {url} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_download(search_term, number_images):\n",
    "    target_folder = '_'.join(search_term.lower().split(' ')) + '/'   # build the target folder path\n",
    "\n",
    "    # make the path if it does not exist\n",
    "    if not os.path.exists(target_folder):   \n",
    "        os.makedirs(target_folder)\n",
    "\n",
    "    # fetch all the images for the search term\n",
    "    with webdriver.Chrome('/usr/bin/chromedriver') as wd:\n",
    "        res = fetch_image_urls(search_term, number_images, wd=wd, sleep_between_interactions=0.5)\n",
    "    \n",
    "    # save every image collected locally\n",
    "    for elem in res:\n",
    "        persist_image(target_folder,elem)\n",
    "        \n",
    "    # move each image save to an S3 bucket and remove the local directory and all its contents\n",
    "    bucket_name = 'actors-images'\n",
    "    file_list = os.listdir(target_folder)\n",
    "    for file in file_list:\n",
    "        s3.upload_file(target_folder + file, bucket_name, target_folder + file)\n",
    " \n",
    "    shutil.rmtree(target_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break down the number of actors being searched for in chucks of 5\n",
    "# for i in range(0, len(actors), 5):\n",
    "for i in range(0, 20, group_size):\n",
    "    sub_actors = actors[i:i+5]\n",
    "    jobs = []\n",
    "    \n",
    "    # create a process for each search term in the sublist\n",
    "    for i in range(len(sub_actors)):\n",
    "        p = multiprocessing.Process(target=search_and_download, args=(sub_actors[i], 10))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "        \n",
    "    # wait for each process to complete\n",
    "    for proc in jobs:\n",
    "        proc.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
